{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b10c8-b03b-49b7-b498-298172d349d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../vendor/graph4nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa373616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from graph4nlp.pytorch.data.dataset import Text2LabelDataset\n",
    "from graph4nlp.pytorch.modules.evaluation.accuracy import Accuracy\n",
    "from graph4nlp.pytorch.modules.graph_construction import ConstituencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction import DependencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction.embedding_construction import WordEmbedding\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GAT\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GraphSAGE\n",
    "from graph4nlp.pytorch.modules.loss.general_loss import GeneralLoss\n",
    "from graph4nlp.pytorch.modules.prediction.classification.graph_classification import FeedForwardNN\n",
    "from graph4nlp.pytorch.modules.utils import constants as Constants\n",
    "from graph4nlp.pytorch.modules.utils.logger import Logger\n",
    "from graph4nlp.pytorch.modules.utils.generic_utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54974454",
   "metadata": {},
   "source": [
    "## Step 1: Define Dataset\n",
    "Retrieve data for training and testing. Visit the movie URL and gather all critic reviews (test set) and about 4 times as many audience reviews (train set). The requests package is used to get the HTML content of the original URLs. BeautifulSoup is used to extract information from the HTML documents. Selenium is used for navigating the website (e.g., clicking buttons and retrieving updated HTML)\n",
    "\n",
    "This demo uses *The Day After Tomorrow* for the movie because it already has a decently even split of positive and negative reviews. This will make it easier to prune the data until it well-balanced.\n",
    "\n",
    "### Data Layout\n",
    "The audience reviews are used for training since there are more, so they go into `train.txt`. The critic reviews are used for testing, so they go into `test.txt`.\n",
    "Each review will be on its own line, followed by a tab and either \"POS\" or \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1ad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTDataset(Text2LabelDataset):\n",
    "    # Define raw and processed file names to prevent NotImplementedError being raised\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\"}\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\"}\n",
    "    \n",
    "    def __init__(self, root_dir, topology_builder=None, topology_subdir=None, graph_type='static',\n",
    "                 pretrained_word_emb_name=\"840B\", pretrained_word_emb_url=None,\n",
    "                 edge_strategy=None, merge_strategy='tailhead', max_word_vocab_size=None,\n",
    "                 min_word_vocab_freq=1, word_emb_size=None, **kwargs):\n",
    "        super(RTDataset, self).__init__(root_dir=root_dir, topology_builder=topology_builder,\n",
    "                                          topology_subdir=topology_subdir, graph_type=graph_type,\n",
    "                                          edge_strategy=edge_strategy, merge_strategy=merge_strategy,\n",
    "                                          max_word_vocab_size=max_word_vocab_size,\n",
    "                                          min_word_vocab_freq=min_word_vocab_freq,\n",
    "                                          pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                                          pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                                          word_emb_size=word_emb_size, **kwargs)\n",
    "    \n",
    "    # Find and click the button that leads to the next page of reviews, and return the HTML source of that page\n",
    "    def _click_next_button(self, driver, button_xpath):\n",
    "        # Make sure the button is loaded on the page before trying to find it\n",
    "        next_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "        next_button.click()\n",
    "        return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "    # Takes in the filepath and an array of strings (each element is a line), and writes each element to the file\n",
    "    def _write_dataset_to_file(self, filepath, text):\n",
    "        # Encode the text as UTF-8\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(text)\n",
    "            \n",
    "    # Balances a train or test file to have equal amounts of positive and negative layers\n",
    "    def _balance_file(self, filepath, num_pos, total):\n",
    "        num_neg = total - num_pos\n",
    "        \n",
    "        # Read all the lines from the current file\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Now, only write back the lines that do not make the set unbalanced\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in lines:\n",
    "                if (num_pos > num_neg and \"\\tPOS\" in line):\n",
    "                    num_pos -= 1\n",
    "                elif (num_neg > num_pos and \"\\tNEG\" in line):\n",
    "                    num_neg -= 1\n",
    "                else:\n",
    "                    f.write(line)\n",
    "\n",
    "    def download(self):\n",
    "        critic_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews\"\n",
    "        critic_req = requests.get(critic_url)\n",
    "        critic_soup = BeautifulSoup(critic_req.content, \"lxml\")\n",
    "        critic_num_pages = 11\n",
    "        critic_total = 0 # The total number of critic reviews\n",
    "        critic_pos = 0 # The number of positive critic reviews\n",
    "        critic_data = []\n",
    "\n",
    "        audience_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews?type=user\"\n",
    "        audience_req = requests.get(audience_url)\n",
    "        audience_soup = BeautifulSoup(audience_req.content, \"lxml\")\n",
    "        audience_num_pages = 200\n",
    "        audience_total = 0 # The total number of audience reviews taken\n",
    "        audience_pos = 0 # The number of positive audience reviews\n",
    "        audience_data = []\n",
    "\n",
    "        driver = Chrome(executable_path=r\"../../bin/chromedriver_win32/chromedriver.exe\")    \n",
    "        driver.get(critic_url)\n",
    "\n",
    "        # All of the review information, including its positive or negative tag, is included in the review_container\n",
    "        # Positive or negative review is determined by fresh or rotten icon\n",
    "        # Review is under class review_text\n",
    "        for page in range(critic_num_pages):\n",
    "            for div in critic_soup.find_all(\"div\", attrs={\"class\": \"col-xs-16 review_container\"}):\n",
    "                text = div.find(\"div\", attrs={\"class\": \"the_review\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    review = \"POS\" if div.find(\"div\", attrs={\"class\": \"review_icon icon small fresh\"}) else \"NEG\"\n",
    "                    critic_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        critic_pos += 1\n",
    "\n",
    "                    critic_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            # Navigate to the next page of reviews\n",
    "            if page < critic_num_pages - 1:\n",
    "                critic_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/div/nav[1]/button[2]\")\n",
    "\n",
    "        critic_freshness = critic_pos / critic_total\n",
    "\n",
    "        driver.get(audience_url)\n",
    "\n",
    "        # For audience reviews, the stars are each icons, either filled, empty, or half, so keep a star count. Then, if the review has >= 3.5 stars, it is positive\n",
    "        for page in range(audience_num_pages):\n",
    "            for div in audience_soup.find_all(\"div\", attrs={\"class\": \"audience-reviews__review-wrap\"}):\n",
    "                text = div.find(\"p\", attrs={\"class\": \"audience-reviews__review js-review-text clamp clamp-8 js-clamp\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    score = len(div.find_all(\"span\", attrs={\"class\": \"star-display__filled\"})) + 0.5 * len(div.find_all(\"span\", attrs={\"class\": \"star-display__half\"}))\n",
    "                    review = \"POS\" if score >= 3.5 else \"NEG\"\n",
    "                    audience_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        audience_pos += 1\n",
    "\n",
    "                    audience_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            if page < audience_num_pages - 1:\n",
    "                audience_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/nav[3]/button[2]\")\n",
    "\n",
    "        audience_freshness = audience_pos / audience_total\n",
    "\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/test.txt\", critic_data)\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/train.txt\", audience_data)\n",
    "        self._balance_file(self.raw_dir + \"/test.txt\", critic_pos, critic_total)\n",
    "        self._balance_file(self.raw_dir + \"/train.txt\", audience_pos, audience_total)\n",
    "\n",
    "        # Write a stats file containing the actual stats retrieved from the web scrape (to be compared against the model's guesses)\n",
    "        with open(self.root + \"actual_stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"## Critic Stats ##\\n\")\n",
    "            f.write(\"Total Critic Reviews: {}\\n\".format(critic_total))\n",
    "            f.write(\"Positive Critic Reviews: {}\\n\".format(critic_pos))\n",
    "            f.write(\"Negative Critic Reviews: {}\\n\".format(critic_total - critic_pos))\n",
    "            f.write(\"Critic Freshness: {:.0%}\\n\\n\".format(critic_freshness))\n",
    "\n",
    "            f.write(\"## Audience Stats ##\\n\")\n",
    "            f.write(\"Total Audience Reviews: {}\\n\".format(audience_total))\n",
    "            f.write(\"Positive Audience Reviews: {}\\n\".format(audience_pos))\n",
    "            f.write(\"Negative Audience Reviews: {}\\n\".format(audience_total - audience_pos))\n",
    "            f.write(\"Audience Freshness: {:.0%}\\n\".format(audience_freshness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112001e-9175-43c5-8eb1-2d20d266811c",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n",
    "The model inherits from Pytorch's nn.Module class. The dataset's vocabulary as well as the configuration data will be passed into it. The configuration data is used to define many parameters of the model, such as the graph type, GNN, learning rate, number of hidden layers, and many more. Note that some values, such as the number of classes in the dataset are added to the configuration automatically and are not defined by the user.\n",
    "\n",
    "The `__init()__` method uses the configuration information to build the graph topology and GNN. It also attaches a feedforward neural network and a loss function.\n",
    "\n",
    "The `forward()` method generates logits (output tensors), and it also returns loss if that is required. This function is does not have to be called manually. This method calculates the gradient descent based on the graph topology, and passes this to the feedforward network to generate logits. The loss is calculated based on the difference between the ouput tensors and target tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d151a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, vocab, config):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        embedding_style = {'single_token_item': True if config['graph_type'] != 'ie' else False,\n",
    "                            'emb_strategy': config.get('emb_strategy', 'w2v_bilstm'),\n",
    "                            'num_rnn_layers': 1,\n",
    "                            'bert_model_name': config.get('bert_model_name', 'bert-base-uncased'),\n",
    "                            'bert_lower_case': True\n",
    "                           }\n",
    "        \n",
    "        if config[\"graph_type\"] == \"constituency\":\n",
    "            self.graph_topology = ConstituencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "        \n",
    "        elif config[\"graph_type\"] == \"dependency\":\n",
    "            self.graph_topology = DependencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(config[\"graph_type\"]))\n",
    "        \n",
    "        \n",
    "        if \"w2v\" in self.graph_topology.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_topology.embedding_layer.word_emb_layers[\"w2v\"].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(self.vocab.in_word_vocab.embeddings.shape[0],\n",
    "                                          self.vocab.in_word_vocab.embeddings.shape[1],\n",
    "                                          pretrained_word_emb=self.vocab.inword_vocab.embeddings,\n",
    "                                          fix_emb=not config[\"no_fix_word_emb\"]\n",
    "                                         ).word_emb_layer\n",
    "        \n",
    "        if config[\"gnn\"] == \"gat\":\n",
    "            heads = [config['gat_num_heads']] * (config['gnn_num_layers'] - 1) + [config['gat_num_out_heads']]\n",
    "            self.gnn = GAT(config['gnn_num_layers'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        heads,\n",
    "                        direction_option=config['gnn_direction_option'],\n",
    "                        feat_drop=config['gnn_dropout'],\n",
    "                        attn_drop=config['gat_attn_dropout'],\n",
    "                        negative_slope=config['gat_negative_slope'],\n",
    "                        residual=config['gat_residual'],\n",
    "                        activation=F.elu)\n",
    "        \n",
    "        elif config[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(config[\"gnn_num_layers\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"graphsage_aggregate_type\"],\n",
    "                                 direction_option=config[\"gnn_direction_option\"],\n",
    "                                 feat_drop=config[\"gnn_dropout\"],\n",
    "                                 bias=True,\n",
    "                                 norm=None,\n",
    "                                 activation=F.relu,\n",
    "                                 use_edge_weight=False)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported gnn type: {}\".format(config[\"gnn\"]))\n",
    "            \n",
    "        self.analyzer = FeedForwardNN(2 * config[\"num_hidden\"] if config[\"gnn_direction_option\"] == \"bi_sep\" else config[\"num_hidden\"],\n",
    "                                        config[\"num_classes\"],\n",
    "                                        [config[\"num_hidden\"]],\n",
    "                                        graph_pool_type=config[\"graph_pooling\"],\n",
    "                                        dim=config[\"num_hidden\"],\n",
    "                                        use_linear_proj=config[\"max_pool_linear_proj\"])\n",
    "        \n",
    "        self.loss = GeneralLoss(\"CrossEntropy\")\n",
    "        \n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        batch_gradient_descent = self.graph_topology(graph_list)\n",
    "        \n",
    "        self.gnn(batch_gradient_descent)\n",
    "        \n",
    "        self.analyzer(batch_gradient_descent)\n",
    "        logits = batch_gradient_descent.graph_attributes[\"logits\"]\n",
    "        \n",
    "        if require_loss:\n",
    "            loss = self.loss(logits, tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6202c-8cf8-41b9-b0bc-a71727b8f313",
   "metadata": {},
   "source": [
    "## Step 3: Define Model Handler\n",
    "The `ModelHandler`  will control the training, evaluation, and testing of the model, and it will provide some utilities to facilitate this. The `ModelHandler` contains the dataset and the model. It defines methods for loading the dataset and creating data loaders for the train, validation, and test sets. It builds the optimizer for the model and defines the evaluation metric. It also defines the `train()`, `evaluate()`, and `test()` functions for the model.\n",
    "\n",
    "The `ModelHandler` also creates a logger to write the performance history of the model to a file. Additionally, it provides training utilites such as an `EarlyStopper`, which will stop the training if the model has not improved since some patience threshold, as well as the ability to reduce the learning rate if the model has not improved recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b603745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def __init__(self, config):\n",
    "        super(ModelHandler, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a logger at the directory specified in the config file.\n",
    "        # The logger will write config and model performance information into a log file\n",
    "        self.logger = Logger(self.config[\"out_dir\"], config={k:v for k, v in self.config.items() if k != \"device\"}, overwrite=True)\n",
    "        self.logger.write(self.config[\"out_dir\"]) # Log config information\n",
    "        \n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "        \n",
    "    def _build_dataloader(self):\n",
    "        if self.config[\"graph_type\"] == \"constituency\":\n",
    "            topology_builder = ConstituencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        elif self.config[\"graph_type\"] == \"dependency\":\n",
    "            topology_builder = DependencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(self.config[\"graph_type\"]))\n",
    "        \n",
    "        topology_subdir = \"{}_graph\".format(self.config[\"graph_type\"])\n",
    "        \n",
    "        dataset = RTDataset(root_dir=\"../../data\",\n",
    "                            pretrained_word_emb_name=self.config.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "                            merge_strategy=merge_strategy,\n",
    "                            seed=self.config[\"seed\"],\n",
    "                            thread_number=4,\n",
    "                            port=9000,\n",
    "                            timeout=15000,\n",
    "                            word_emb_size=300,\n",
    "                            graph_type=graph_type, topology_builder=topology_builder,\n",
    "                            topology_subdir=topology_subdir,\n",
    "                            dynamic_graph_type=None,\n",
    "                            dynamic_init_topology_builder=None,\n",
    "                            dynamic_init_topology_aux_args={\"dummy_param\": 0})\n",
    "        \n",
    "        self.train_dataloader = DataLoader(dataset.train, batch_size=self.config[\"batch_size\"], \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=self.config[\"num_workers\"],\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        # Create a validation set if one is specified. If not, the validation set is the same as the test set\n",
    "        if hasattr(dataset, \"val\") == False:\n",
    "            dataset.val = dataset.test\n",
    "        self.val_dataloader = DataLoader(dataset.val, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset.test, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.vocab = dataset.vocab_model\n",
    "        self.config[\"num_classes\"] = dataset.num_classes\n",
    "        self.num_train = len(dataset.train)\n",
    "        self.num_val = len(dataset.val)\n",
    "        self.num_test = len(dataset.test)\n",
    "        print(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        self.logger.write(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        \n",
    "    # Build the semantic analyzer and put it on the GPU if it is available\n",
    "    def _build_model(self):\n",
    "        self.model = SentimentAnalyzer(self.vocab, self.config).to(self.config[\"device\"])\n",
    "        \n",
    "    # Define the optimzer and helpers for the optimzer.\n",
    "    # The stopper allows the model to stop training if it has not improved in a while\n",
    "    # The scheduler allows the learning rate to be decreased if the model plateaus\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.config[\"lr\"])\n",
    "        self.stopper = EarlyStopping(os.path.join(self.config[\"out_dir\"], Constants._SAVED_WEIGHTS_FILE), patience=self.config[\"patience\"])\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"max\", factor=self.config[\"lr_reduce_factor\"],\n",
    "                                           patience=self.config[\"lr_patience\"], verbose=True)\n",
    "    \n",
    "    # Since the model can only be right or wrong, accuracy is the best performance metric\n",
    "    def _build_evaluation(self):\n",
    "        self.metric = Accuracy([\"accuracy\"])\n",
    "    \n",
    "    def train(self):\n",
    "        dur = []\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            t0 = time.time()\n",
    "            for i, data in enumerate(self.train_dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits, loss = self.model(data[\"graph_data\"], tgt, require_loss=True)\n",
    "                \n",
    "                # Add graph regularization loss if available\n",
    "                # Regularizing the graph introduces more loss (the regularization factor)\n",
    "                if data[\"graph_data\"].graph_attributes.get(\"graph_reg\", None) is not None:\n",
    "                    loss = loss + data[\"graph_data\"].graph_attributes[\"graph_reg\"]\n",
    "                \n",
    "                # Backpropgation step\n",
    "                # Zero the gradients, take the derivative of the loss, and step the optimizer\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                # Make a prediction based on the logits calculated earlier\n",
    "                predict = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(self.metric.calculate_scores(ground_truth=tgt.cpu(), predict=predict.cpu(), zero_division=0)[0])\n",
    "                dur.append(time.time() - t0)\n",
    "                \n",
    "            val_acc = self.evaluate(self.val_dataloader)\n",
    "            self.scheduler.step(val_acc)\n",
    "            print(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            self.logger.write(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            \n",
    "            if self.stopper.step(val_acc, self.model):\n",
    "                break\n",
    "        \n",
    "        return self.stopper.best_score\n",
    "    \n",
    "    # Validation step of the model\n",
    "    # Used within model training at the end of each epoch\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_collect = []\n",
    "            tgt_collect = []\n",
    "            \n",
    "            for i, data in enumerate(dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits = self.model(data[\"graph_data\"], require_loss=False)\n",
    "                pred_collect.append(logits)\n",
    "                tgt_collect.append(tgt)\n",
    "            \n",
    "            pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "            tgt_collect = torch.cat(tgt_collect, 0).cpu()\n",
    "            score = self.metric.calculate_scores(ground_truth=tgt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "            \n",
    "            return score\n",
    "        \n",
    "    def test(self):\n",
    "        # Restore the best saved model\n",
    "        self.stopper.load_checkpoint(self.model)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        acc = self.evaluate(self.test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        self.logger.write(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5aabdd-fce7-46ff-98c8-a555757b5f38",
   "metadata": {},
   "source": [
    "## Step 4: Configure and Run the Model\n",
    "Open the configuration file, which will be fed to the model through the `ModelHandler`. The configuration file describes the parameters of how the model should be built. So, to get a different model, the only thing that needs to change is to provide a different configuration file.RNG seeds for numpy and PyTorch are set so that the model training is deterministic (will have the same result every time). Also, add to the configuration the device the model should run on.\n",
    "\n",
    "The model will detect whether raw or processed data already exists. If there is no raw or processed data, `download()` method of `RTDataset` will be called automatically. If there is raw data but no processed data, StanfordCoreNLP needs to be running at the same time as this notebook with the same port and timeout that is defined in `RTDataset` (in this case, port 9000 with timeout=15000). To run StanfordCoreNLP (v4.2.2) with Java 8, navigate to the folder that contains StanfordCoreNLP and run this command:\n",
    "\n",
    "    java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "\n",
    "Finally, run the `train()` and `test()` functions and see the how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1c290b-e7a8-44f9-a212-bbc87f870a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CPU ]\n",
      "\n",
      "../../out/rotten_tomatoes/graphsage_bi_fuse_constituency_ckpt\n",
      "Building vocabs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.840B.300d.zip: 2.18GB [30:39, 1.18MB/s]                                                            \n",
      "100%|████████████████████████████████████████████████████████████████████▉| 2196016/2196017 [03:14<00:00, 11288.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained word embeddings hit ratio: 0.9436234263820471\n",
      "Using pretrained word embeddings\n",
      "[ Initialized word embeddings: (3654, 300) ]\n",
      "Saving vocab model to ../../data\\processed\\constituency_graph\\vocab.pt\n",
      "Loading pre-built vocab model stored in ../../data\\processed\\constituency_graph\\vocab.pt\n",
      "Train size: 1771, Val size: 160, Test size: 160\n",
      "[ Fix word embeddings ]\n",
      "Epoch: [1 / 500] | Time: 52.68s | Loss: 0.6468 | Train Acc: 60.11% | Val Acc: 53.75%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_constituency_ckpt\\params.saved\n",
      "Epoch: [2 / 500] | Time: 51.93s | Loss: 0.5775 | Train Acc: 70.23% | Val Acc: 52.50%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [3 / 500] | Time: 50.83s | Loss: 0.5498 | Train Acc: 72.87% | Val Acc: 52.50%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch: [4 / 500] | Time: 51.11s | Loss: 0.5368 | Train Acc: 74.30% | Val Acc: 50.62%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [5 / 500] | Time: 51.14s | Loss: 0.5191 | Train Acc: 75.57% | Val Acc: 53.12%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: [6 / 500] | Time: 51.42s | Loss: 0.4996 | Train Acc: 75.13% | Val Acc: 50.62%\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: [7 / 500] | Time: 51.94s | Loss: 0.4897 | Train Acc: 75.92% | Val Acc: 53.12%\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: [8 / 500] | Time: 51.82s | Loss: 0.4726 | Train Acc: 77.04% | Val Acc: 52.50%\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: [9 / 500] | Time: 51.30s | Loss: 0.4639 | Train Acc: 77.27% | Val Acc: 51.88%\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch: [10 / 500] | Time: 50.99s | Loss: 0.4499 | Train Acc: 79.36% | Val Acc: 56.25%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_constituency_ckpt\\params.saved\n",
      "Epoch: [11 / 500] | Time: 50.99s | Loss: 0.4435 | Train Acc: 78.98% | Val Acc: 53.12%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [12 / 500] | Time: 51.19s | Loss: 0.4251 | Train Acc: 79.88% | Val Acc: 51.88%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    13: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: [13 / 500] | Time: 51.17s | Loss: 0.4074 | Train Acc: 80.94% | Val Acc: 50.62%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [14 / 500] | Time: 51.20s | Loss: 0.3908 | Train Acc: 81.95% | Val Acc: 50.62%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: [15 / 500] | Time: 51.04s | Loss: 0.3774 | Train Acc: 82.71% | Val Acc: 50.00%\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch    16: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: [16 / 500] | Time: 51.11s | Loss: 0.3921 | Train Acc: 81.52% | Val Acc: 53.12%\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: [17 / 500] | Time: 51.14s | Loss: 0.3542 | Train Acc: 84.35% | Val Acc: 52.50%\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: [18 / 500] | Time: 50.93s | Loss: 0.3719 | Train Acc: 82.58% | Val Acc: 52.50%\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch    19: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: [19 / 500] | Time: 50.71s | Loss: 0.3518 | Train Acc: 82.86% | Val Acc: 51.25%\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: [20 / 500] | Time: 50.79s | Loss: 0.3492 | Train Acc: 83.62% | Val Acc: 50.00%\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Test examples: 160 | Time: 14.51s | Test Acc: 56.25%\n",
      "Total runtime: 1953.46s\n",
      "val acc: 0.5625, test acc: 0.5625\n"
     ]
    }
   ],
   "source": [
    "# Configure\n",
    "config_file = \"../../config/graph4nlp/graphsage_bi_fuse_static_constituency.yaml\"\n",
    "config = yaml.load(open(config_file, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Set all RNG seeds to the same seed to ensure a deterministic model\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "if not config[\"no_cuda\"] and torch.cuda.is_available():\n",
    "    print(\"[ Using CUDA ]\")\n",
    "    config[\"device\"] = torch.device(\"cuda\" if config[\"gpu\"] < 0 else \"cuda:%d\" % config[\"gpu\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"[ Using CPU ]\")\n",
    "    config[\"device\"] = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"\\n\" + config[\"out_dir\"])\n",
    "\n",
    "runner = ModelHandler(config)\n",
    "t0 = time.time()\n",
    "\n",
    "val_acc = runner.train()\n",
    "test_acc = runner.test()\n",
    "\n",
    "runtime = time.time() - t0\n",
    "print('Total runtime: {:.2f}s'.format(runtime))\n",
    "runner.logger.write('Total runtime: {:.2f}s\\n'.format(runtime))\n",
    "runner.logger.close()\n",
    "\n",
    "print('val acc: {}, test acc: {}'.format(val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b95e3-1cfc-4ba4-bac0-c19dbefa2396",
   "metadata": {},
   "source": [
    "## Results\n",
    "Constituency: 69.5% accuracy\n",
    "\n",
    "Dependency: 62.2% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee87ec-c3a6-4cbc-b035-2868cbbac9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

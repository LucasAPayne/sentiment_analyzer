{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b10c8-b03b-49b7-b498-298172d349d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../vendor/graph4nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa373616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "from graph4nlp.pytorch.data.dataset import Text2LabelDataset\n",
    "from graph4nlp.pytorch.modules.evaluation.accuracy import Accuracy\n",
    "from graph4nlp.pytorch.modules.graph_construction import ConstituencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction import DependencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction.embedding_construction import WordEmbedding\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GAT\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GraphSAGE\n",
    "from graph4nlp.pytorch.modules.loss.general_loss import GeneralLoss\n",
    "from graph4nlp.pytorch.modules.prediction.classification.graph_classification import FeedForwardNN\n",
    "from graph4nlp.pytorch.modules.utils import constants as Constants\n",
    "from graph4nlp.pytorch.modules.utils.logger import Logger\n",
    "from graph4nlp.pytorch.modules.utils.generic_utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54974454",
   "metadata": {},
   "source": [
    "## Step 1: Define Datasets\n",
    "## Custom Rotten Tomatoes Dataset\n",
    "Retrieve data for training and testing. Visit the movie URL and gather all critic reviews (test set) and about 4 times as many audience reviews (train set). The requests package is used to get the HTML content of the original URLs. BeautifulSoup is used to extract information from the HTML documents. Selenium is used for navigating the website (e.g., clicking buttons and retrieving updated HTML)\n",
    "\n",
    "This demo uses *The Day After Tomorrow* for the movie because it already has a decently even split of positive and negative reviews. This will make it easier to prune the data until it well-balanced.\n",
    "\n",
    "### Data Layout\n",
    "The audience reviews are used for training since there are more, so they go into `train.txt`. The critic reviews are used for testing, so they go into `test.txt`.\n",
    "Each review will be on its own line, followed by a tab and either \"POS\" or \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1ad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTDataset(Text2LabelDataset):\n",
    "    # Define raw and processed file names to prevent NotImplementedError being raised\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\"}\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\"}\n",
    "    \n",
    "    def __init__(self, root_dir, topology_builder=None, topology_subdir=None, graph_type='static',\n",
    "                 pretrained_word_emb_name=\"840B\", pretrained_word_emb_url=None,\n",
    "                 edge_strategy=None, merge_strategy='tailhead', max_word_vocab_size=None,\n",
    "                 min_word_vocab_freq=1, word_emb_size=None, **kwargs):\n",
    "        super(RTDataset, self).__init__(root_dir=root_dir, topology_builder=topology_builder,\n",
    "                                          topology_subdir=topology_subdir, graph_type=graph_type,\n",
    "                                          edge_strategy=edge_strategy, merge_strategy=merge_strategy,\n",
    "                                          max_word_vocab_size=max_word_vocab_size,\n",
    "                                          min_word_vocab_freq=min_word_vocab_freq,\n",
    "                                          pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                                          pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                                          word_emb_size=word_emb_size, **kwargs)\n",
    "    \n",
    "    # Find and click the button that leads to the next page of reviews, and return the HTML source of that page\n",
    "    def _click_next_button(self, driver, button_xpath):\n",
    "        # Make sure the button is loaded on the page before trying to find it\n",
    "        next_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "        next_button.click()\n",
    "        return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "    # Takes in the filepath and an array of strings (each element is a line), and writes each element to the file\n",
    "    def _write_dataset_to_file(self, filepath, text):\n",
    "        # Encode the text as UTF-8\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(text)\n",
    "            \n",
    "    # Balances a train or test file to have equal amounts of positive and negative layers\n",
    "    def _balance_file(self, filepath, num_pos, total):\n",
    "        num_neg = total - num_pos\n",
    "        \n",
    "        # Read all the lines from the current file\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Now, only write back the lines that do not make the set unbalanced\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in lines:\n",
    "                if (num_pos > num_neg and \"\\tPOS\" in line):\n",
    "                    num_pos -= 1\n",
    "                elif (num_neg > num_pos and \"\\tNEG\" in line):\n",
    "                    num_neg -= 1\n",
    "                else:\n",
    "                    f.write(line)\n",
    "\n",
    "    # Scrape Rotten Tomatoes to create a custom dataset. Called automatically by Text2LabelDataset base class if raw data is not already available                \n",
    "    def download(self):\n",
    "        critic_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews\"\n",
    "        critic_req = requests.get(critic_url)\n",
    "        critic_soup = BeautifulSoup(critic_req.content, \"lxml\")\n",
    "        critic_num_pages = 11\n",
    "        critic_total = 0 # The total number of critic reviews\n",
    "        critic_pos = 0 # The number of positive critic reviews\n",
    "        critic_data = []\n",
    "\n",
    "        audience_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews?type=user\"\n",
    "        audience_req = requests.get(audience_url)\n",
    "        audience_soup = BeautifulSoup(audience_req.content, \"lxml\")\n",
    "        audience_num_pages = 200\n",
    "        audience_total = 0 # The total number of audience reviews taken\n",
    "        audience_pos = 0 # The number of positive audience reviews\n",
    "        audience_data = []\n",
    "\n",
    "        driver = Chrome(executable_path=r\"../../bin/chromedriver_win32/chromedriver.exe\")    \n",
    "        driver.get(critic_url)\n",
    "\n",
    "        # All of the review information, including its positive or negative tag, is included in the review_container\n",
    "        # Positive or negative review is determined by fresh or rotten icon\n",
    "        # Review is under class review_text\n",
    "        for page in range(critic_num_pages):\n",
    "            for div in critic_soup.find_all(\"div\", attrs={\"class\": \"col-xs-16 review_container\"}):\n",
    "                text = div.find(\"div\", attrs={\"class\": \"the_review\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    review = \"POS\" if div.find(\"div\", attrs={\"class\": \"review_icon icon small fresh\"}) else \"NEG\"\n",
    "                    critic_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        critic_pos += 1\n",
    "\n",
    "                    critic_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            # Navigate to the next page of reviews\n",
    "            if page < critic_num_pages - 1:\n",
    "                critic_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/div/nav[1]/button[2]\")\n",
    "\n",
    "        critic_freshness = critic_pos / critic_total\n",
    "\n",
    "        driver.get(audience_url)\n",
    "\n",
    "        # For audience reviews, the stars are each icons, either filled, empty, or half, so keep a star count. Then, if the review has >= 3.5 stars, it is positive\n",
    "        for page in range(audience_num_pages):\n",
    "            for div in audience_soup.find_all(\"div\", attrs={\"class\": \"audience-reviews__review-wrap\"}):\n",
    "                text = div.find(\"p\", attrs={\"class\": \"audience-reviews__review js-review-text clamp clamp-8 js-clamp\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    score = len(div.find_all(\"span\", attrs={\"class\": \"star-display__filled\"})) + 0.5 * len(div.find_all(\"span\", attrs={\"class\": \"star-display__half\"}))\n",
    "                    review = \"POS\" if score >= 3.5 else \"NEG\"\n",
    "                    audience_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        audience_pos += 1\n",
    "\n",
    "                    audience_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            if page < audience_num_pages - 1:\n",
    "                audience_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/nav[3]/button[2]\")\n",
    "\n",
    "        driver.quit() # Close the browser\n",
    "        audience_freshness = audience_pos / audience_total\n",
    "\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/test.txt\", critic_data)\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/train.txt\", audience_data)\n",
    "        self._balance_file(self.raw_dir + \"/test.txt\", critic_pos, critic_total)\n",
    "        self._balance_file(self.raw_dir + \"/train.txt\", audience_pos, audience_total)\n",
    "\n",
    "        # Write a stats file containing the actual stats retrieved from the web scrape (to be compared against the model's guesses)\n",
    "        with open(self.root + \"/actual_stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"## Critic Stats ##\\n\")\n",
    "            f.write(\"Total Critic Reviews: {}\\n\".format(critic_total))\n",
    "            f.write(\"Positive Critic Reviews: {}\\n\".format(critic_pos))\n",
    "            f.write(\"Negative Critic Reviews: {}\\n\".format(critic_total - critic_pos))\n",
    "            f.write(\"Critic Freshness: {:.0%}\\n\\n\".format(critic_freshness))\n",
    "\n",
    "            f.write(\"## Audience Stats ##\\n\")\n",
    "            f.write(\"Total Audience Reviews: {}\\n\".format(audience_total))\n",
    "            f.write(\"Positive Audience Reviews: {}\\n\".format(audience_pos))\n",
    "            f.write(\"Negative Audience Reviews: {}\\n\".format(audience_total - audience_pos))\n",
    "            f.write(\"Audience Freshness: {:.0%}\\n\".format(audience_freshness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c6bdf-353c-4734-9e1c-da5df7c2c429",
   "metadata": {},
   "source": [
    "## Torchtext IMBD Dataset\n",
    "Take the IMDB dataset provided by torchtext and put it in the format that Graph4NLP expects. By default, it is divided into test and train sets, with each having folders for positive and negative reviews. The test and train sets each have 25,000 reviews, and the entire dataset has 50,000. Each review is in its own file, and they are not labeled.\n",
    "\n",
    "To transform this dataset into a Text2LabelDataset, combine the separate files, where each line consists of a review followed by a tab followed by either \"POS\" or \"NEG\". Three files are made here: `train.txt`, `test.txt`, and `val.txt`. The dataset is constructed in the `download()` method so that it is only fetched if it is not already present. Finally, once the dataset has been condensed, delete all of the individual files as well as the extra/unnecessary files that accompany the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eee87ec-c3a6-4cbc-b035-2868cbbac9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Text2LabelDataset):\n",
    "    # Define raw and processed file names to prevent NotImplementedError being raised\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\", \"val\": \"val.txt\"}\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\"}\n",
    "    \n",
    "    def __init__(self, root_dir, topology_builder=None, topology_subdir=None, graph_type='static',\n",
    "                 pretrained_word_emb_name=\"840B\", pretrained_word_emb_url=None,\n",
    "                 edge_strategy=None, merge_strategy='tailhead', max_word_vocab_size=None,\n",
    "                 min_word_vocab_freq=1, word_emb_size=None, **kwargs):\n",
    "        super(IMDBDataset, self).__init__(root_dir=root_dir, topology_builder=topology_builder,\n",
    "                                          topology_subdir=topology_subdir, graph_type=graph_type,\n",
    "                                          edge_strategy=edge_strategy, merge_strategy=merge_strategy,\n",
    "                                          max_word_vocab_size=max_word_vocab_size,\n",
    "                                          min_word_vocab_freq=min_word_vocab_freq,\n",
    "                                          pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                                          pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                                          word_emb_size=word_emb_size, **kwargs)\n",
    "\n",
    "    # pos_neg is either \"pos\" or \"neg\", to find the right directory and write the appropriate label\n",
    "    def _gather_lines(self, test_train, pos_neg):\n",
    "        root_dir = \"../../data/imdb/IMDB/aclImdb/\" + test_train + \"/\" + pos_neg.lower()\n",
    "        lines = []\n",
    "        for entry in os.scandir(root_dir):\n",
    "            with open(entry.path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.readline().rstrip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "                cutoff = raw_text.find(\".\", 500) # Find the index of the first period after the 500th character to limit the length of the string\n",
    "                text = raw_text[:cutoff] if len(raw_text) > cutoff else raw_text\n",
    "                text += '\\t' + pos_neg.upper() + '\\n'\n",
    "                lines.append(text)\n",
    "                \n",
    "            # Prevent training set from being larger than 12,500 entries (6,250 positive and 6,250 negative)\n",
    "            if test_train == \"train\" and len(lines) >= 2500:\n",
    "                return lines\n",
    "        return lines\n",
    "\n",
    "    def _write_lines_to_file(self, filepath, lines):\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    # Remove old/unnecessary files and folders from the dataset\n",
    "    def _clean_dataset(self):\n",
    "        shutil.move(\"../../data/imdb/IMDB/aclImdb/README\", \"../../data/imdb/README.md\")\n",
    "        shutil.rmtree(\"../../data/imdb/IMDB\")\n",
    "\n",
    "    # The IMDB dataset from torchtext organizes its files differently from what we want (different test/train and pos/neg directories with entries being individual files)\n",
    "    # Walk through the directories, combine into just a test and train file formatted properly for Text2LabelDataset\n",
    "    # test_train_val is either \"test\", \"train\", or \"val\" to loop through the train directory or the test directory (used for both \"test\" and \"val\")\n",
    "    def _condense_dataset(self, test_train):\n",
    "        new_file = \"../../data/imdb/raw/\" + test_train + \".txt\"\n",
    "        lines = []\n",
    "        lines.extend(self._gather_lines(test_train, \"pos\"))\n",
    "        lines.extend(self._gather_lines(test_train, \"neg\"))\n",
    "\n",
    "        # The desired split is train = 70%, test/val = 15% each\n",
    "        # Since test set is 12,500 entries, test/val should be 2,500 entries each\n",
    "        if test_train == \"test\":\n",
    "            # The dataset is well-defined at 25,000 test reviews, so using magic numbers here to avoid complicated calculations\n",
    "            # The validation set is the first set of 1250 positive and 1250 negative reviews, and the test set is the second set of 1250 positive and 1250 negative reviews\n",
    "            val_lines = lines[:500]\n",
    "            val_lines.extend(lines[12501:13001])\n",
    "            test_lines = lines[2500:3000]\n",
    "            test_lines.extend(lines[13001:13501])\n",
    "            lines = test_lines\n",
    "            random.shuffle(val_lines)\n",
    "            self._write_lines_to_file(\"../../data/imdb/raw/val.txt\", val_lines)\n",
    "        \n",
    "        random.shuffle(lines)\n",
    "        self._write_lines_to_file(new_file, lines)\n",
    "            \n",
    "    def download(self):\n",
    "        train_data, test_data = IMDB(root=\"../../data/imdb\")\n",
    "        self._condense_dataset(\"train\")\n",
    "        self._condense_dataset(\"test\")\n",
    "        self._clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112001e-9175-43c5-8eb1-2d20d266811c",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n",
    "The model inherits from Pytorch's nn.Module class. The dataset's vocabulary as well as the configuration data will be passed into it. The configuration data is used to define many parameters of the model, such as the graph type, GNN, learning rate, number of hidden layers, and many more. Note that some values, such as the number of classes in the dataset are added to the configuration automatically and are not defined by the user.\n",
    "\n",
    "The `__init()__` method uses the configuration information to build the graph topology and GNN. It also attaches a feedforward neural network and a loss function.\n",
    "\n",
    "The `forward()` method generates logits (output tensors), and it also returns loss if that is required. This function is does not have to be called manually. This method calculates the gradient descent based on the graph topology, and passes this to the feedforward network to generate logits. The loss is calculated based on the difference between the ouput tensors and target tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d151a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, vocab, config):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        embedding_style = {'single_token_item': True if config['graph_type'] != 'ie' else False,\n",
    "                            'emb_strategy': config.get('emb_strategy', 'w2v_bilstm'),\n",
    "                            'num_rnn_layers': 1,\n",
    "                            'bert_model_name': config.get('bert_model_name', 'bert-base-uncased'),\n",
    "                            'bert_lower_case': True\n",
    "                           }\n",
    "        \n",
    "        if config[\"graph_type\"] == \"constituency\":\n",
    "            self.graph_topology = ConstituencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "        \n",
    "        elif config[\"graph_type\"] == \"dependency\":\n",
    "            self.graph_topology = DependencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(config[\"graph_type\"]))\n",
    "        \n",
    "        \n",
    "        if \"w2v\" in self.graph_topology.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_topology.embedding_layer.word_emb_layers[\"w2v\"].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(self.vocab.in_word_vocab.embeddings.shape[0],\n",
    "                                          self.vocab.in_word_vocab.embeddings.shape[1],\n",
    "                                          pretrained_word_emb=self.vocab.inword_vocab.embeddings,\n",
    "                                          fix_emb=not config[\"no_fix_word_emb\"]\n",
    "                                         ).word_emb_layer\n",
    "        \n",
    "        if config[\"gnn\"] == \"gat\":\n",
    "            heads = [config['gat_num_heads']] * (config['gnn_num_layers'] - 1) + [config['gat_num_out_heads']]\n",
    "            self.gnn = GAT(config['gnn_num_layers'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        heads,\n",
    "                        direction_option=config['gnn_direction_option'],\n",
    "                        feat_drop=config['gnn_dropout'],\n",
    "                        attn_drop=config['gat_attn_dropout'],\n",
    "                        negative_slope=config['gat_negative_slope'],\n",
    "                        residual=config['gat_residual'],\n",
    "                        activation=F.elu)\n",
    "        \n",
    "        elif config[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(config[\"gnn_num_layers\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"graphsage_aggregate_type\"],\n",
    "                                 direction_option=config[\"gnn_direction_option\"],\n",
    "                                 feat_drop=config[\"gnn_dropout\"],\n",
    "                                 bias=True,\n",
    "                                 norm=None,\n",
    "                                 activation=F.relu,\n",
    "                                 use_edge_weight=False)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported gnn type: {}\".format(config[\"gnn\"]))\n",
    "            \n",
    "        self.analyzer = FeedForwardNN(2 * config[\"num_hidden\"] if config[\"gnn_direction_option\"] == \"bi_sep\" else config[\"num_hidden\"],\n",
    "                                        config[\"num_classes\"],\n",
    "                                        [config[\"num_hidden\"]],\n",
    "                                        graph_pool_type=config[\"graph_pooling\"],\n",
    "                                        dim=config[\"num_hidden\"],\n",
    "                                        use_linear_proj=config[\"max_pool_linear_proj\"])\n",
    "        \n",
    "        self.loss = GeneralLoss(\"CrossEntropy\")\n",
    "        \n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        batch_gradient_descent = self.graph_topology(graph_list)\n",
    "        \n",
    "        self.gnn(batch_gradient_descent)\n",
    "        \n",
    "        self.analyzer(batch_gradient_descent)\n",
    "        logits = batch_gradient_descent.graph_attributes[\"logits\"]\n",
    "        \n",
    "        if require_loss:\n",
    "            loss = self.loss(logits, tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6202c-8cf8-41b9-b0bc-a71727b8f313",
   "metadata": {},
   "source": [
    "## Step 3: Define Model Handler\n",
    "The `ModelHandler`  will control the training, evaluation, and testing of the model, and it will provide some utilities to facilitate this. The `ModelHandler` contains the dataset and the model. It defines methods for loading the dataset and creating data loaders for the train, validation, and test sets. It builds the optimizer for the model and defines the evaluation metric. It also defines the `train()`, `evaluate()`, and `test()` functions for the model.\n",
    "\n",
    "The `ModelHandler` also creates a logger to write the performance history of the model to a file. Additionally, it provides training utilites such as an `EarlyStopper`, which will stop the training if the model has not improved since some patience threshold, as well as the ability to reduce the learning rate if the model has not improved recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b603745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def __init__(self, config):\n",
    "        super(ModelHandler, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a logger at the directory specified in the config file.\n",
    "        # The logger will write config and model performance information into a log file\n",
    "        self.logger = Logger(self.config[\"out_dir\"], config={k:v for k, v in self.config.items() if k != \"device\"}, overwrite=True)\n",
    "        self.logger.write(self.config[\"out_dir\"]) # Log config information\n",
    "        \n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "        \n",
    "    def _build_dataloader(self):\n",
    "        if self.config[\"graph_type\"] == \"constituency\":\n",
    "            topology_builder = ConstituencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        elif self.config[\"graph_type\"] == \"dependency\":\n",
    "            topology_builder = DependencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(self.config[\"graph_type\"]))\n",
    "        \n",
    "        topology_subdir = \"{}_graph\".format(self.config[\"graph_type\"])\n",
    "        \n",
    "        if self.config[\"dataset\"] == \"rotten_tomatoes\":\n",
    "            dataset = RTDataset(root_dir=\"../../data/rotten_tomatoes\",\n",
    "                                pretrained_word_emb_name=self.config.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "                                merge_strategy=merge_strategy,\n",
    "                                seed=self.config[\"seed\"],\n",
    "                                thread_number=4,\n",
    "                                port=9000,\n",
    "                                timeout=50000,\n",
    "                                word_emb_size=300,\n",
    "                                graph_type=graph_type, topology_builder=topology_builder,\n",
    "                                topology_subdir=topology_subdir,\n",
    "                                dynamic_graph_type=None,\n",
    "                                dynamic_init_topology_builder=None,\n",
    "                                dynamic_init_topology_aux_args={\"dummy_param\": 0})\n",
    "            \n",
    "        elif self.config[\"dataset\"] == \"imdb\":\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "            dataset = IMDBDataset(root_dir=\"../../data/imdb\",\n",
    "                                pretrained_word_emb_name=self.config.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "                                merge_strategy=merge_strategy,\n",
    "                                seed=self.config[\"seed\"],\n",
    "                                thread_number=4,\n",
    "                                port=9000,\n",
    "                                timeout=50000,\n",
    "                                word_emb_size=300,\n",
    "                                graph_type=graph_type, topology_builder=topology_builder,\n",
    "                                topology_subdir=topology_subdir,\n",
    "                                dynamic_graph_type=None,\n",
    "                                dynamic_init_topology_builder=None,\n",
    "                                dynamic_init_topology_aux_args={\"dummy_param\": 0})\n",
    "        \n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown dataset: {}\".format(self.config[\"dataset\"]))\n",
    "        \n",
    "        self.train_dataloader = DataLoader(dataset.train, batch_size=self.config[\"batch_size\"], \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=self.config[\"num_workers\"],\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        # Create a validation set if one is specified. If not, the validation set is the same as the test set\n",
    "        if hasattr(dataset, \"val\") == False:\n",
    "            dataset.val = dataset.test\n",
    "        self.val_dataloader = DataLoader(dataset.val, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset.test, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.vocab = dataset.vocab_model\n",
    "        self.config[\"num_classes\"] = dataset.num_classes\n",
    "        self.num_train = len(dataset.train)\n",
    "        self.num_val = len(dataset.val)\n",
    "        self.num_test = len(dataset.test)\n",
    "        print(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        self.logger.write(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        \n",
    "    # Build the semantic analyzer and put it on the GPU if it is available\n",
    "    def _build_model(self):\n",
    "        self.model = SentimentAnalyzer(self.vocab, self.config).to(self.config[\"device\"])\n",
    "        \n",
    "    # Define the optimzer and helpers for the optimzer.\n",
    "    # The stopper allows the model to stop training if it has not improved in a while\n",
    "    # The scheduler allows the learning rate to be decreased if the model plateaus\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.config[\"lr\"])\n",
    "        self.stopper = EarlyStopping(os.path.join(self.config[\"out_dir\"], Constants._SAVED_WEIGHTS_FILE), patience=self.config[\"patience\"])\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"max\", factor=self.config[\"lr_reduce_factor\"],\n",
    "                                           patience=self.config[\"lr_patience\"], verbose=True)\n",
    "    \n",
    "    # Since the model can only be right or wrong, accuracy is the best performance metric\n",
    "    def _build_evaluation(self):\n",
    "        self.metric = Accuracy([\"accuracy\"])\n",
    "    \n",
    "    def train(self):\n",
    "        dur = []\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            t0 = time.time()\n",
    "            for i, data in enumerate(self.train_dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits, loss = self.model(data[\"graph_data\"], tgt, require_loss=True)\n",
    "                \n",
    "                # Add graph regularization loss if available\n",
    "                # Regularizing the graph introduces more loss (the regularization factor)\n",
    "                if data[\"graph_data\"].graph_attributes.get(\"graph_reg\", None) is not None:\n",
    "                    loss = loss + data[\"graph_data\"].graph_attributes[\"graph_reg\"]\n",
    "                \n",
    "                # Backpropgation step\n",
    "                # Zero the gradients, take the derivative of the loss, and step the optimizer\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                # Make a prediction based on the logits calculated earlier\n",
    "                predict = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(self.metric.calculate_scores(ground_truth=tgt.cpu(), predict=predict.cpu(), zero_division=0)[0])\n",
    "                dur.append(time.time() - t0)\n",
    "                \n",
    "            val_acc = self.evaluate(self.val_dataloader)\n",
    "            self.scheduler.step(val_acc)\n",
    "            print(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            self.logger.write(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            \n",
    "            if self.stopper.step(val_acc, self.model):\n",
    "                break\n",
    "        \n",
    "        return self.stopper.best_score\n",
    "    \n",
    "    # Validation step of the model\n",
    "    # Used within model training at the end of each epoch\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_collect = []\n",
    "            tgt_collect = []\n",
    "            \n",
    "            for i, data in enumerate(dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits = self.model(data[\"graph_data\"], require_loss=False)\n",
    "                pred_collect.append(logits)\n",
    "                tgt_collect.append(tgt)\n",
    "            \n",
    "            pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "            tgt_collect = torch.cat(tgt_collect, 0).cpu()\n",
    "            score = self.metric.calculate_scores(ground_truth=tgt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "            \n",
    "            return score\n",
    "        \n",
    "    def test(self):\n",
    "        # Restore the best saved model\n",
    "        self.stopper.load_checkpoint(self.model)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        acc = self.evaluate(self.test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        self.logger.write(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5aabdd-fce7-46ff-98c8-a555757b5f38",
   "metadata": {},
   "source": [
    "## Step 4: Configure and Run the Model\n",
    "Open the configuration file, which will be fed to the model through the `ModelHandler`. The configuration file describes the parameters of how the model should be built. So, to get a different model, the only thing that needs to change is to provide a different configuration file.RNG seeds for numpy and PyTorch are set so that the model training is deterministic (will have the same result every time). Also, add to the configuration the device the model should run on.\n",
    "\n",
    "The model will detect whether raw or processed data already exists. If there is no raw or processed data, `download()` method of `RTDataset` will be called automatically. If there is raw data but no processed data, StanfordCoreNLP needs to be running at the same time as this notebook with the same port and timeout that is defined in `RTDataset` (in this case, port 9000 with timeout=15000). To run StanfordCoreNLP (v4.2.2) with Java 8, navigate to the folder that contains StanfordCoreNLP and run this command:\n",
    "\n",
    "    java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 50000\n",
    "\n",
    "Finally, run the `train()` and `test()` functions and see the how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1c290b-e7a8-44f9-a212-bbc87f870a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CUDA ]\n",
      "\n",
      "../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabs...\n",
      "Pretrained word embeddings hit ratio: 0.8629453845095612\n",
      "Using pretrained word embeddings\n",
      "[ Initialized word embeddings: (29076, 300) ]\n",
      "Saving vocab model to ../../data/imdb\\processed\\dependency_graph\\vocab.pt\n",
      "Loading pre-built vocab model stored in ../../data/imdb\\processed\\dependency_graph\\vocab.pt\n",
      "Train size: 4962, Val size: 992, Test size: 992\n",
      "[ Fix word embeddings ]\n",
      "Epoch: [1 / 500] | Time: 37.25s | Loss: 0.6216 | Train Acc: 64.59% | Val Acc: 71.47%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [2 / 500] | Time: 31.55s | Loss: 0.5098 | Train Acc: 74.92% | Val Acc: 76.11%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [3 / 500] | Time: 29.29s | Loss: 0.4808 | Train Acc: 76.50% | Val Acc: 76.11%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [4 / 500] | Time: 28.15s | Loss: 0.4560 | Train Acc: 78.71% | Val Acc: 80.04%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [5 / 500] | Time: 27.43s | Loss: 0.4383 | Train Acc: 79.66% | Val Acc: 79.54%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [6 / 500] | Time: 26.88s | Loss: 0.4250 | Train Acc: 79.98% | Val Acc: 80.85%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [7 / 500] | Time: 26.50s | Loss: 0.4053 | Train Acc: 81.11% | Val Acc: 80.44%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [8 / 500] | Time: 26.24s | Loss: 0.3860 | Train Acc: 82.25% | Val Acc: 82.06%\n",
      "Saved model to ../../out/rotten_tomatoes/graphsage_bi_fuse_dependency_ckpt\\params.saved\n",
      "Epoch: [9 / 500] | Time: 26.04s | Loss: 0.3644 | Train Acc: 83.34% | Val Acc: 80.34%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch: [10 / 500] | Time: 25.88s | Loss: 0.3390 | Train Acc: 84.51% | Val Acc: 80.95%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: [11 / 500] | Time: 25.75s | Loss: 0.3354 | Train Acc: 84.87% | Val Acc: 80.95%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch: [12 / 500] | Time: 25.64s | Loss: 0.2921 | Train Acc: 86.76% | Val Acc: 80.75%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch: [13 / 500] | Time: 25.63s | Loss: 0.2724 | Train Acc: 88.34% | Val Acc: 81.05%\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch    14: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: [14 / 500] | Time: 25.58s | Loss: 0.2561 | Train Acc: 88.96% | Val Acc: 80.44%\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch: [15 / 500] | Time: 25.53s | Loss: 0.2268 | Train Acc: 90.72% | Val Acc: 80.85%\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch: [16 / 500] | Time: 25.47s | Loss: 0.2078 | Train Acc: 91.19% | Val Acc: 79.94%\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch    17: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: [17 / 500] | Time: 25.41s | Loss: 0.2012 | Train Acc: 91.64% | Val Acc: 80.24%\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch: [18 / 500] | Time: 25.36s | Loss: 0.1902 | Train Acc: 91.96% | Val Acc: 80.24%\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Test examples: 992 | Time: 9.26s | Test Acc: 82.86%\n",
      "Total runtime: 871.40s\n",
      "val acc: 0.8205645161290323, test acc: 0.8286290322580645\n"
     ]
    }
   ],
   "source": [
    "# Configure\n",
    "config_file = \"../../config/graph4nlp/graphsage_bi_fuse_static_dependency.yaml\"\n",
    "config = yaml.load(open(config_file, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Set all RNG seeds to the same seed to ensure a deterministic model\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "if not config[\"no_cuda\"] and torch.cuda.is_available():\n",
    "    print(\"[ Using CUDA ]\")\n",
    "    config[\"device\"] = torch.device(\"cuda\" if config[\"gpu\"] < 0 else \"cuda:%d\" % config[\"gpu\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"[ Using CPU ]\")\n",
    "    config[\"device\"] = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"\\n\" + config[\"out_dir\"])\n",
    "\n",
    "runner = ModelHandler(config)\n",
    "t0 = time.time()\n",
    "\n",
    "val_acc = runner.train()\n",
    "test_acc = runner.test()\n",
    "\n",
    "runtime = time.time() - t0\n",
    "print('Total runtime: {:.2f}s'.format(runtime))\n",
    "runner.logger.write('Total runtime: {:.2f}s\\n'.format(runtime))\n",
    "runner.logger.close()\n",
    "\n",
    "print('val acc: {}, test acc: {}'.format(val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b95e3-1cfc-4ba4-bac0-c19dbefa2396",
   "metadata": {},
   "source": [
    "## Best Results\n",
    "### Rotten Tomatoes\n",
    "- Constituency: 69.5% accuracy\n",
    "- Dependency: 62.2% accuracy\n",
    "\n",
    "### IMDB\n",
    "- Constituency: 83.1% accuracy\n",
    "- Dependency: 82.9% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c4847-4543-41ed-ac0c-896195868cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

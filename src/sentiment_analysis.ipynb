{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95b10c8-b03b-49b7-b498-298172d349d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../vendor/graph4nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa373616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import stanfordcorenlp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "from graph4nlp.pytorch.data.data import to_batch\n",
    "from graph4nlp.pytorch.data.dataset import Text2LabelDataItem, Text2LabelDataset\n",
    "from graph4nlp.pytorch.modules.evaluation.accuracy import Accuracy\n",
    "from graph4nlp.pytorch.modules.graph_construction import ConstituencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction import DependencyBasedGraphConstruction\n",
    "from graph4nlp.pytorch.modules.graph_construction.embedding_construction import WordEmbedding\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GAT\n",
    "from graph4nlp.pytorch.modules.graph_embedding import GraphSAGE\n",
    "from graph4nlp.pytorch.modules.loss.general_loss import GeneralLoss\n",
    "from graph4nlp.pytorch.modules.prediction.classification.graph_classification import FeedForwardNN\n",
    "from graph4nlp.pytorch.modules.utils import constants as Constants\n",
    "from graph4nlp.pytorch.modules.utils.logger import Logger\n",
    "from graph4nlp.pytorch.modules.utils.generic_utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54974454",
   "metadata": {},
   "source": [
    "## Step 1: Define Datasets\n",
    "## Custom Rotten Tomatoes Dataset\n",
    "Retrieve data for training and testing. Visit the movie URL and gather all critic reviews (test set) and about 4 times as many audience reviews (train set). The requests package is used to get the HTML content of the original URLs. BeautifulSoup is used to extract information from the HTML documents. Selenium is used for navigating the website (e.g., clicking buttons and retrieving updated HTML)\n",
    "\n",
    "This demo uses *The Day After Tomorrow* for the movie because it already has a decently even split of positive and negative reviews. This will make it easier to prune the data until it well-balanced.\n",
    "\n",
    "### Data Layout\n",
    "The audience reviews are used for training since there are more, so they go into `train.txt`. The critic reviews are used for testing, so they go into `test.txt`.\n",
    "Each review will be on its own line, followed by a tab and either \"POS\" or \"NEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1ad98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTDataset(Text2LabelDataset):\n",
    "    # Define raw and processed file names to prevent NotImplementedError being raised\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\"}\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\"}\n",
    "    \n",
    "    def __init__(self, root_dir, topology_builder=None, topology_subdir=None, graph_type='static',\n",
    "                 pretrained_word_emb_name=\"840B\", pretrained_word_emb_url=None,\n",
    "                 edge_strategy=None, merge_strategy='tailhead', max_word_vocab_size=None,\n",
    "                 min_word_vocab_freq=1, word_emb_size=None, **kwargs):\n",
    "        super(RTDataset, self).__init__(root_dir=root_dir, topology_builder=topology_builder,\n",
    "                                          topology_subdir=topology_subdir, graph_type=graph_type,\n",
    "                                          edge_strategy=edge_strategy, merge_strategy=merge_strategy,\n",
    "                                          max_word_vocab_size=max_word_vocab_size,\n",
    "                                          min_word_vocab_freq=min_word_vocab_freq,\n",
    "                                          pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                                          pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                                          word_emb_size=word_emb_size, **kwargs)\n",
    "    \n",
    "    # Find and click the button that leads to the next page of reviews, and return the HTML source of that page\n",
    "    def _click_next_button(self, driver, button_xpath):\n",
    "        # Make sure the button is loaded on the page before trying to find it\n",
    "        next_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "        next_button.click()\n",
    "        return BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "    # Takes in the filepath and an array of strings (each element is a line), and writes each element to the file\n",
    "    def _write_dataset_to_file(self, filepath, text):\n",
    "        # Encode the text as UTF-8\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(text)\n",
    "            \n",
    "    # Balances a train or test file to have equal amounts of positive and negative layers\n",
    "    def _balance_file(self, filepath, num_pos, total):\n",
    "        num_neg = total - num_pos\n",
    "        \n",
    "        # Read all the lines from the current file\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Now, only write back the lines that do not make the set unbalanced\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in lines:\n",
    "                if (num_pos > num_neg and \"\\tPOS\" in line):\n",
    "                    num_pos -= 1\n",
    "                elif (num_neg > num_pos and \"\\tNEG\" in line):\n",
    "                    num_neg -= 1\n",
    "                else:\n",
    "                    f.write(line)\n",
    "\n",
    "    # Scrape Rotten Tomatoes to create a custom dataset. Called automatically by Text2LabelDataset base class if raw data is not already available                \n",
    "    def download(self):\n",
    "        critic_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews\"\n",
    "        critic_req = requests.get(critic_url)\n",
    "        critic_soup = BeautifulSoup(critic_req.content, \"lxml\")\n",
    "        critic_num_pages = 11\n",
    "        critic_total = 0 # The total number of critic reviews\n",
    "        critic_pos = 0 # The number of positive critic reviews\n",
    "        critic_data = []\n",
    "\n",
    "        audience_url = \"https://www.rottentomatoes.com/m/day_after_tomorrow/reviews?type=user\"\n",
    "        audience_req = requests.get(audience_url)\n",
    "        audience_soup = BeautifulSoup(audience_req.content, \"lxml\")\n",
    "        audience_num_pages = 200\n",
    "        audience_total = 0 # The total number of audience reviews taken\n",
    "        audience_pos = 0 # The number of positive audience reviews\n",
    "        audience_data = []\n",
    "\n",
    "        driver = Chrome(executable_path=r\"../bin/chromedriver_win32/chromedriver.exe\")    \n",
    "        driver.get(critic_url)\n",
    "\n",
    "        # All of the review information, including its positive or negative tag, is included in the review_container\n",
    "        # Positive or negative review is determined by fresh or rotten icon\n",
    "        # Review is under class review_text\n",
    "        for page in range(critic_num_pages):\n",
    "            for div in critic_soup.find_all(\"div\", attrs={\"class\": \"col-xs-16 review_container\"}):\n",
    "                text = div.find(\"div\", attrs={\"class\": \"the_review\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    review = \"POS\" if div.find(\"div\", attrs={\"class\": \"review_icon icon small fresh\"}) else \"NEG\"\n",
    "                    critic_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        critic_pos += 1\n",
    "\n",
    "                    critic_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            # Navigate to the next page of reviews\n",
    "            if page < critic_num_pages - 1:\n",
    "                critic_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/div/nav[1]/button[2]\")\n",
    "\n",
    "        critic_freshness = critic_pos / critic_total\n",
    "\n",
    "        driver.get(audience_url)\n",
    "\n",
    "        # For audience reviews, the stars are each icons, either filled, empty, or half, so keep a star count. Then, if the review has >= 3.5 stars, it is positive\n",
    "        for page in range(audience_num_pages):\n",
    "            for div in audience_soup.find_all(\"div\", attrs={\"class\": \"audience-reviews__review-wrap\"}):\n",
    "                text = div.find(\"p\", attrs={\"class\": \"audience-reviews__review js-review-text clamp clamp-8 js-clamp\"}).text.strip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "\n",
    "                if text:\n",
    "                    score = len(div.find_all(\"span\", attrs={\"class\": \"star-display__filled\"})) + 0.5 * len(div.find_all(\"span\", attrs={\"class\": \"star-display__half\"}))\n",
    "                    review = \"POS\" if score >= 3.5 else \"NEG\"\n",
    "                    audience_total += 1\n",
    "\n",
    "                    if review == \"POS\":\n",
    "                        audience_pos += 1\n",
    "\n",
    "                    audience_data.append(text + '\\t' + review + '\\n')\n",
    "\n",
    "            if page < audience_num_pages - 1:\n",
    "                audience_soup = self._click_next_button(driver, \"//*[@id='content']/div/div/nav[3]/button[2]\")\n",
    "\n",
    "        driver.quit() # Close the browser\n",
    "        audience_freshness = audience_pos / audience_total\n",
    "\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/test.txt\", critic_data)\n",
    "        self._write_dataset_to_file(self.raw_dir + \"/train.txt\", audience_data)\n",
    "        self._balance_file(self.raw_dir + \"/test.txt\", critic_pos, critic_total)\n",
    "        self._balance_file(self.raw_dir + \"/train.txt\", audience_pos, audience_total)\n",
    "\n",
    "        # Write a stats file containing the actual stats retrieved from the web scrape (to be compared against the model's guesses)\n",
    "        with open(self.root + \"/actual_stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"## Critic Stats ##\\n\")\n",
    "            f.write(\"Total Critic Reviews: {}\\n\".format(critic_total))\n",
    "            f.write(\"Positive Critic Reviews: {}\\n\".format(critic_pos))\n",
    "            f.write(\"Negative Critic Reviews: {}\\n\".format(critic_total - critic_pos))\n",
    "            f.write(\"Critic Freshness: {:.0%}\\n\\n\".format(critic_freshness))\n",
    "\n",
    "            f.write(\"## Audience Stats ##\\n\")\n",
    "            f.write(\"Total Audience Reviews: {}\\n\".format(audience_total))\n",
    "            f.write(\"Positive Audience Reviews: {}\\n\".format(audience_pos))\n",
    "            f.write(\"Negative Audience Reviews: {}\\n\".format(audience_total - audience_pos))\n",
    "            f.write(\"Audience Freshness: {:.0%}\\n\".format(audience_freshness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c6bdf-353c-4734-9e1c-da5df7c2c429",
   "metadata": {},
   "source": [
    "## Torchtext IMBD Dataset\n",
    "Take the IMDB dataset provided by torchtext and put it in the format that Graph4NLP expects. By default, it is divided into test and train sets, with each having folders for positive and negative reviews. The test and train sets each have 25,000 reviews, and the entire dataset has 50,000. Each review is in its own file, and they are not labeled.\n",
    "\n",
    "To transform this dataset into a Text2LabelDataset, combine the separate files, where each line consists of a review followed by a tab followed by either \"POS\" or \"NEG\". Three files are made here: `train.txt`, `test.txt`, and `val.txt`. The dataset is constructed in the `download()` method so that it is only fetched if it is not already present. Finally, once the dataset has been condensed, delete all of the individual files as well as the extra/unnecessary files that accompany the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eee87ec-c3a6-4cbc-b035-2868cbbac9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Text2LabelDataset):\n",
    "    # Define raw and processed file names to prevent NotImplementedError being raised\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return {\"train\": \"train.txt\", \"test\": \"test.txt\", \"val\": \"val.txt\"}\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return {\"vocab\": \"vocab.pt\", \"data\": \"data.pt\"}\n",
    "    \n",
    "    def __init__(self, root_dir, topology_builder=None, topology_subdir=None, graph_type='static',\n",
    "                 pretrained_word_emb_name=\"840B\", pretrained_word_emb_url=None,\n",
    "                 edge_strategy=None, merge_strategy='tailhead', max_word_vocab_size=None,\n",
    "                 min_word_vocab_freq=1, word_emb_size=None, **kwargs):\n",
    "        super(IMDBDataset, self).__init__(root_dir=root_dir, topology_builder=topology_builder,\n",
    "                                          topology_subdir=topology_subdir, graph_type=graph_type,\n",
    "                                          edge_strategy=edge_strategy, merge_strategy=merge_strategy,\n",
    "                                          max_word_vocab_size=max_word_vocab_size,\n",
    "                                          min_word_vocab_freq=min_word_vocab_freq,\n",
    "                                          pretrained_word_emb_name=pretrained_word_emb_name,\n",
    "                                          pretrained_word_emb_url=pretrained_word_emb_url,\n",
    "                                          word_emb_size=word_emb_size, **kwargs)\n",
    "\n",
    "    # pos_neg is either \"pos\" or \"neg\", to find the right directory and write the appropriate label\n",
    "    def _gather_lines(self, test_train, pos_neg):\n",
    "        root_dir = \"../data/imdb/IMDB/aclImdb/\" + test_train + \"/\" + pos_neg.lower()\n",
    "        lines = []\n",
    "        for entry in os.scandir(root_dir):\n",
    "            with open(entry.path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.readline().rstrip().replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "                cutoff = raw_text.find(\".\", 500) # Find the index of the first period after the 500th character to limit the length of the string\n",
    "                text = raw_text[:cutoff] if len(raw_text) > cutoff else raw_text\n",
    "                text += '\\t' + pos_neg.upper() + '\\n'\n",
    "                lines.append(text)\n",
    "                \n",
    "            # Prevent training set from being larger than 12,500 entries (6,250 positive and 6,250 negative)\n",
    "            if test_train == \"train\" and len(lines) >= 2500:\n",
    "                return lines\n",
    "        return lines\n",
    "\n",
    "    def _write_lines_to_file(self, filepath, lines):\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(lines)\n",
    "\n",
    "    # Remove old/unnecessary files and folders from the dataset\n",
    "    def _clean_dataset(self):\n",
    "        shutil.move(\"../data/imdb/IMDB/aclImdb/README\", \"../data/imdb/README.md\")\n",
    "        shutil.rmtree(\"../data/imdb/IMDB\")\n",
    "\n",
    "    # The IMDB dataset from torchtext organizes its files differently from what we want (different test/train and pos/neg directories with entries being individual files)\n",
    "    # Walk through the directories, combine into just a test and train file formatted properly for Text2LabelDataset\n",
    "    # test_train_val is either \"test\", \"train\", or \"val\" to loop through the train directory or the test directory (used for both \"test\" and \"val\")\n",
    "    def _condense_dataset(self, test_train):\n",
    "        new_file = \"../data/imdb/raw/\" + test_train + \".txt\"\n",
    "        lines = []\n",
    "        lines.extend(self._gather_lines(test_train, \"pos\"))\n",
    "        lines.extend(self._gather_lines(test_train, \"neg\"))\n",
    "\n",
    "        # The desired split is train = 70%, test/val = 15% each\n",
    "        # Since test set is 12,500 entries, test/val should be 2,500 entries each\n",
    "        if test_train == \"test\":\n",
    "            # The dataset is well-defined at 25,000 test reviews, so using magic numbers here to avoid complicated calculations\n",
    "            # The validation set is the first set of 1250 positive and 1250 negative reviews, and the test set is the second set of 1250 positive and 1250 negative reviews\n",
    "            val_lines = lines[:500]\n",
    "            val_lines.extend(lines[12501:13001])\n",
    "            test_lines = lines[2500:3000]\n",
    "            test_lines.extend(lines[13001:13501])\n",
    "            lines = test_lines\n",
    "            random.shuffle(val_lines)\n",
    "            self._write_lines_to_file(\"../data/imdb/raw/val.txt\", val_lines)\n",
    "        \n",
    "        random.shuffle(lines)\n",
    "        self._write_lines_to_file(new_file, lines)\n",
    "            \n",
    "    def download(self):\n",
    "        train_data, test_data = IMDB(root=\"../data/imdb\")\n",
    "        self._condense_dataset(\"train\")\n",
    "        self._condense_dataset(\"test\")\n",
    "        self._clean_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112001e-9175-43c5-8eb1-2d20d266811c",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n",
    "The model inherits from Pytorch's nn.Module class. The dataset's vocabulary as well as the configuration data will be passed into it. The configuration data is used to define many parameters of the model, such as the graph type, GNN, learning rate, number of hidden layers, and many more. Note that some values, such as the number of classes in the dataset are added to the configuration automatically and are not defined by the user.\n",
    "\n",
    "The `__init()__` method uses the configuration information to build the graph topology and GNN. It also attaches a feedforward neural network and a loss function.\n",
    "\n",
    "The `forward()` method generates logits (output tensors), and it also returns loss if that is required. This function is does not have to be called manually. This method calculates the gradient descent based on the graph topology, and passes this to the feedforward network to generate logits. The loss is calculated based on the difference between the ouput tensors and target tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d151a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    def __init__(self, vocab, config):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab = vocab\n",
    "        embedding_style = {'single_token_item': True if config['graph_type'] != 'ie' else False,\n",
    "                            'emb_strategy': config.get('emb_strategy', 'w2v_bilstm'),\n",
    "                            'num_rnn_layers': 1,\n",
    "                            'bert_model_name': config.get('bert_model_name', 'bert-base-uncased'),\n",
    "                            'bert_lower_case': True\n",
    "                           }\n",
    "        \n",
    "        if config[\"graph_type\"] == \"constituency\":\n",
    "            self.graph_topology = ConstituencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "        \n",
    "        elif config[\"graph_type\"] == \"dependency\":\n",
    "            self.graph_topology = DependencyBasedGraphConstruction(embedding_style=embedding_style,\n",
    "                                                                     vocab=vocab.in_word_vocab,\n",
    "                                                                     hidden_size=config[\"num_hidden\"],\n",
    "                                                                     word_dropout=config[\"word_dropout\"],\n",
    "                                                                     rnn_dropout=config[\"rnn_dropout\"],\n",
    "                                                                     fix_word_emb=not config[\"no_fix_word_emb\"],\n",
    "                                                                     fix_bert_emb=not config.get(\"no_fix_bert_emb\", False))\n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(config[\"graph_type\"]))\n",
    "        \n",
    "        \n",
    "        if \"w2v\" in self.graph_topology.embedding_layer.word_emb_layers:\n",
    "            self.word_emb = self.graph_topology.embedding_layer.word_emb_layers[\"w2v\"].word_emb_layer\n",
    "        else:\n",
    "            self.word_emb = WordEmbedding(self.vocab.in_word_vocab.embeddings.shape[0],\n",
    "                                          self.vocab.in_word_vocab.embeddings.shape[1],\n",
    "                                          pretrained_word_emb=self.vocab.inword_vocab.embeddings,\n",
    "                                          fix_emb=not config[\"no_fix_word_emb\"]\n",
    "                                         ).word_emb_layer\n",
    "        \n",
    "        if config[\"gnn\"] == \"gat\":\n",
    "            heads = [config['gat_num_heads']] * (config['gnn_num_layers'] - 1) + [config['gat_num_out_heads']]\n",
    "            self.gnn = GAT(config['gnn_num_layers'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        config['num_hidden'],\n",
    "                        heads,\n",
    "                        direction_option=config['gnn_direction_option'],\n",
    "                        feat_drop=config['gnn_dropout'],\n",
    "                        attn_drop=config['gat_attn_dropout'],\n",
    "                        negative_slope=config['gat_negative_slope'],\n",
    "                        residual=config['gat_residual'],\n",
    "                        activation=F.elu)\n",
    "        \n",
    "        elif config[\"gnn\"] == \"graphsage\":\n",
    "            self.gnn = GraphSAGE(config[\"gnn_num_layers\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"num_hidden\"],\n",
    "                                 config[\"graphsage_aggregate_type\"],\n",
    "                                 direction_option=config[\"gnn_direction_option\"],\n",
    "                                 feat_drop=config[\"gnn_dropout\"],\n",
    "                                 bias=True,\n",
    "                                 norm=None,\n",
    "                                 activation=F.relu,\n",
    "                                 use_edge_weight=False)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported gnn type: {}\".format(config[\"gnn\"]))\n",
    "            \n",
    "        self.analyzer = FeedForwardNN(2 * config[\"num_hidden\"] if config[\"gnn_direction_option\"] == \"bi_sep\" else config[\"num_hidden\"],\n",
    "                                        config[\"num_classes\"],\n",
    "                                        [config[\"num_hidden\"]],\n",
    "                                        graph_pool_type=config[\"graph_pooling\"],\n",
    "                                        dim=config[\"num_hidden\"],\n",
    "                                        use_linear_proj=config[\"max_pool_linear_proj\"])\n",
    "        \n",
    "        self.loss = GeneralLoss(\"CrossEntropy\")\n",
    "        \n",
    "    def forward(self, graph_list, tgt=None, require_loss=True):\n",
    "        batch_gradient_descent = self.graph_topology(graph_list)\n",
    "        \n",
    "        self.gnn(batch_gradient_descent)\n",
    "        \n",
    "        self.analyzer(batch_gradient_descent)\n",
    "        logits = batch_gradient_descent.graph_attributes[\"logits\"]\n",
    "        \n",
    "        if require_loss:\n",
    "            loss = self.loss(logits, tgt)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6202c-8cf8-41b9-b0bc-a71727b8f313",
   "metadata": {},
   "source": [
    "## Step 3: Define Model Handler\n",
    "The `ModelHandler`  will control the training, evaluation, and testing of the model, and it will provide some utilities to facilitate this. The `ModelHandler` contains the dataset and the model. It defines methods for loading the dataset and creating data loaders for the train, validation, and test sets. It builds the optimizer for the model and defines the evaluation metric. It also defines the `train()`, `evaluate()`, and `test()` functions for the model.\n",
    "\n",
    "The `ModelHandler` also creates a logger to write the performance history of the model to a file. Additionally, it provides training utilites such as an `EarlyStopper`, which will stop the training if the model has not improved since some patience threshold, as well as the ability to reduce the learning rate if the model has not improved recently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b603745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    def __init__(self, config):\n",
    "        super(ModelHandler, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Create a logger at the directory specified in the config file.\n",
    "        # The logger will write config and model performance information into a log file\n",
    "        self.logger = Logger(self.config[\"out_dir\"], config={k:v for k, v in self.config.items() if k != \"device\"}, overwrite=True)\n",
    "        self.logger.write(self.config[\"out_dir\"]) # Log config information\n",
    "        \n",
    "        self._build_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        self._build_evaluation()\n",
    "        \n",
    "    def _build_dataloader(self):\n",
    "        if self.config[\"graph_type\"] == \"constituency\":\n",
    "            topology_builder = ConstituencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        elif self.config[\"graph_type\"] == \"dependency\":\n",
    "            topology_builder = DependencyBasedGraphConstruction\n",
    "            graph_type = \"static\"\n",
    "            merge_strategy = \"tailhead\"\n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown/unsupported graph_type: {}\".format(self.config[\"graph_type\"]))\n",
    "        \n",
    "        topology_subdir = \"{}_graph\".format(self.config[\"graph_type\"])\n",
    "        \n",
    "        if self.config[\"dataset\"] == \"rotten_tomatoes\":\n",
    "            dataset = RTDataset(root_dir=\"../data/rotten_tomatoes\",\n",
    "                                pretrained_word_emb_name=self.config.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "                                merge_strategy=merge_strategy,\n",
    "                                seed=self.config[\"seed\"],\n",
    "                                thread_number=4,\n",
    "                                port=9000,\n",
    "                                timeout=50000,\n",
    "                                word_emb_size=300,\n",
    "                                graph_type=graph_type, topology_builder=topology_builder,\n",
    "                                topology_subdir=topology_subdir,\n",
    "                                dynamic_graph_type=None,\n",
    "                                dynamic_init_topology_builder=None,\n",
    "                                dynamic_init_topology_aux_args={\"dummy_param\": 0})\n",
    "            \n",
    "        elif self.config[\"dataset\"] == \"imdb\":\n",
    "            import nltk\n",
    "            nltk.download(\"punkt\")\n",
    "            dataset = IMDBDataset(root_dir=\"../data/imdb\",\n",
    "                                pretrained_word_emb_name=self.config.get(\"pretrained_word_emb_name\", \"840B\"),\n",
    "                                merge_strategy=merge_strategy,\n",
    "                                seed=self.config[\"seed\"],\n",
    "                                thread_number=4,\n",
    "                                port=9000,\n",
    "                                timeout=50000,\n",
    "                                word_emb_size=300,\n",
    "                                graph_type=graph_type, topology_builder=topology_builder,\n",
    "                                topology_subdir=topology_subdir,\n",
    "                                dynamic_graph_type=None,\n",
    "                                dynamic_init_topology_builder=None,\n",
    "                                dynamic_init_topology_aux_args={\"dummy_param\": 0})\n",
    "        \n",
    "        else:\n",
    "            raise RuntimeError(\"Unknown dataset: {}\".format(self.config[\"dataset\"]))\n",
    "        \n",
    "        self.train_dataloader = DataLoader(dataset.train, batch_size=self.config[\"batch_size\"], \n",
    "                                           shuffle=True,\n",
    "                                           num_workers=self.config[\"num_workers\"],\n",
    "                                           collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        # Create a validation set if one is specified. If not, the validation set is the same as the test set\n",
    "        if hasattr(dataset, \"val\") == False:\n",
    "            dataset.val = dataset.test\n",
    "        self.val_dataloader = DataLoader(dataset.val, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.test_dataloader = DataLoader(dataset.test, batch_size=self.config[\"batch_size\"],\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=self.config[\"num_workers\"],\n",
    "                                         collate_fn=dataset.collate_fn)\n",
    "        \n",
    "        self.vocab = dataset.vocab_model\n",
    "        self.config[\"num_classes\"] = dataset.num_classes\n",
    "        self.num_train = len(dataset.train)\n",
    "        self.num_val = len(dataset.val)\n",
    "        self.num_test = len(dataset.test)\n",
    "        print(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        self.logger.write(\"Train size: {}, Val size: {}, Test size: {}\"\n",
    "              .format(self.num_train, self.num_val, self.num_test))\n",
    "        \n",
    "    # Build the semantic analyzer and put it on the GPU if it is available\n",
    "    def _build_model(self):\n",
    "        self.model = SentimentAnalyzer(self.vocab, self.config).to(self.config[\"device\"])\n",
    "        \n",
    "    # Define the optimzer and helpers for the optimzer.\n",
    "    # The stopper allows the model to stop training if it has not improved in a while\n",
    "    # The scheduler allows the learning rate to be decreased if the model plateaus\n",
    "    def _build_optimizer(self):\n",
    "        parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.Adam(parameters, lr=self.config[\"lr\"])\n",
    "        self.stopper = EarlyStopping(os.path.join(self.config[\"out_dir\"], Constants._SAVED_WEIGHTS_FILE), patience=self.config[\"patience\"])\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"max\", factor=self.config[\"lr_reduce_factor\"],\n",
    "                                           patience=self.config[\"lr_patience\"], verbose=True)\n",
    "    \n",
    "    # Since the model can only be right or wrong, accuracy is the best performance metric\n",
    "    def _build_evaluation(self):\n",
    "        self.metric = Accuracy([\"accuracy\"])\n",
    "    \n",
    "    def train(self):\n",
    "        dur = []\n",
    "        for epoch in range(self.config[\"epochs\"]):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            t0 = time.time()\n",
    "            for i, data in enumerate(self.train_dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits, loss = self.model(data[\"graph_data\"], tgt, require_loss=True)\n",
    "                \n",
    "                # Add graph regularization loss if available\n",
    "                # Regularizing the graph introduces more loss (the regularization factor)\n",
    "                if data[\"graph_data\"].graph_attributes.get(\"graph_reg\", None) is not None:\n",
    "                    loss = loss + data[\"graph_data\"].graph_attributes[\"graph_reg\"]\n",
    "                \n",
    "                # Backpropgation step\n",
    "                # Zero the gradients, take the derivative of the loss, and step the optimizer\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                # Make a prediction based on the logits calculated earlier\n",
    "                predict = torch.max(logits, dim=-1)[1].cpu()\n",
    "                train_acc.append(self.metric.calculate_scores(ground_truth=tgt.cpu(), predict=predict.cpu(), zero_division=0)[0])\n",
    "                dur.append(time.time() - t0)\n",
    "                \n",
    "            val_acc = self.evaluate(self.val_dataloader)\n",
    "            self.scheduler.step(val_acc)\n",
    "            print(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            self.logger.write(\"Epoch: [{} / {}] | Time: {:.2f}s | Loss: {:.4f} | Train Acc: {:.2%} | Val Acc: {:.2%}\"\n",
    "                  .format(epoch + 1, self.config[\"epochs\"], np.mean(dur), np.mean(train_loss), np.mean(train_acc), val_acc))\n",
    "            \n",
    "            if self.stopper.step(val_acc, self.model):\n",
    "                break\n",
    "        \n",
    "        return self.stopper.best_score\n",
    "    \n",
    "    # Validation step of the model\n",
    "    # Used within model training at the end of each epoch\n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_collect = []\n",
    "            tgt_collect = []\n",
    "            \n",
    "            for i, data in enumerate(dataloader):\n",
    "                tgt = data['tgt_tensor'].to(self.config['device'])\n",
    "                data[\"graph_data\"] = data[\"graph_data\"].to(self.config[\"device\"])\n",
    "                logits = self.model(data[\"graph_data\"], require_loss=False)\n",
    "                pred_collect.append(logits)\n",
    "                tgt_collect.append(tgt)\n",
    "            \n",
    "            pred_collect = torch.max(torch.cat(pred_collect, 0), dim=-1)[1].cpu()\n",
    "            tgt_collect = torch.cat(tgt_collect, 0).cpu()\n",
    "            score = self.metric.calculate_scores(ground_truth=tgt_collect, predict=pred_collect, zero_division=0)[0]\n",
    "            \n",
    "            return score\n",
    "        \n",
    "    def test(self):\n",
    "        # Restore the best saved model\n",
    "        self.stopper.load_checkpoint(self.model)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        acc = self.evaluate(self.test_dataloader)\n",
    "        dur = time.time() - t0\n",
    "        print(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        self.logger.write(\"Test examples: {} | Time: {:.2f}s | Test Acc: {:.2%}\"\n",
    "              .format(self.num_test, dur, acc))\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5aabdd-fce7-46ff-98c8-a555757b5f38",
   "metadata": {},
   "source": [
    "## Step 4: Configure and Run the Model\n",
    "Open the configuration file, which will be fed to the model through the `ModelHandler`. The configuration file describes the parameters of how the model should be built. So, to get a different model, the only thing that needs to change is to provide a different configuration file.RNG seeds for numpy and PyTorch are set so that the model training is deterministic (will have the same result every time). Also, add to the configuration the device the model should run on.\n",
    "\n",
    "The model will detect whether raw or processed data already exists. If there is no raw or processed data, `download()` method of `RTDataset` will be called automatically. If there is raw data but no processed data, StanfordCoreNLP needs to be running at the same time as this notebook with the same port and timeout that is defined in `RTDataset` (in this case, port 9000 with timeout=15000). To run StanfordCoreNLP (v4.2.2) with Java 8, navigate to the folder that contains StanfordCoreNLP and run this command:\n",
    "\n",
    "    java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 50000\n",
    "\n",
    "Finally, run the `train()` and `test()` functions and see the how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c290b-e7a8-44f9-a212-bbc87f870a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using CPU ]\n",
      "\n",
      "../out/imdb/graphsage_bi_fuse_dependency_ckpt\n"
     ]
    }
   ],
   "source": [
    "# Configure\n",
    "config_file = \"../config/graphsage_bi_fuse_static_dependency.yaml\"\n",
    "config = yaml.load(open(config_file, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# Set all RNG seeds to the same seed to ensure a deterministic model\n",
    "np.random.seed(config[\"seed\"])\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "if not config[\"no_cuda\"] and torch.cuda.is_available():\n",
    "    print(\"[ Using CUDA ]\")\n",
    "    config[\"device\"] = torch.device(\"cuda\" if config[\"gpu\"] < 0 else \"cuda:%d\" % config[\"gpu\"])\n",
    "    torch.cuda.manual_seed(config[\"seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"seed\"])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"[ Using CPU ]\")\n",
    "    config[\"device\"] = torch.device(\"cpu\")\n",
    "    \n",
    "print(\"\\n\" + config[\"out_dir\"])\n",
    "\n",
    "runner = ModelHandler(config)\n",
    "t0 = time.time()\n",
    "\n",
    "val_acc = runner.train()\n",
    "test_acc = runner.test()\n",
    "\n",
    "runtime = time.time() - t0\n",
    "print('Total runtime: {:.2f}s'.format(runtime))\n",
    "runner.logger.write('Total runtime: {:.2f}s\\n'.format(runtime))\n",
    "runner.logger.close()\n",
    "\n",
    "print('val acc: {}, test acc: {}'.format(val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b95e3-1cfc-4ba4-bac0-c19dbefa2396",
   "metadata": {},
   "source": [
    "## Best Results\n",
    "### Rotten Tomatoes\n",
    "- Constituency: 69.5% accuracy\n",
    "- Dependency: 62.2% accuracy\n",
    "\n",
    "### IMDB\n",
    "- Constituency: 83.1% accuracy\n",
    "- Dependency: 82.9% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c4847-4543-41ed-ac0c-896195868cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = runner.stopper.load_checkpoint(runner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9263f87-9613-45d0-b31b-461c7fe57268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topology_builder(config):\n",
    "    if config[\"graph_type\"] == \"constituency\":\n",
    "        topology_builder = ConstituencyBasedGraphConstruction\n",
    "    \n",
    "    elif config[\"graph_type\"] == \"dependency\":\n",
    "        topology_builder = DependencyBasedGraphConstruction\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(\"Unkown graph_type: {}\".format(config[\"graph_type\"]))\n",
    "                           \n",
    "    return topology_builder\n",
    "\n",
    "def get_processor_args(topology_builder):\n",
    "    if topology_builder == ConstituencyBasedGraphConstruction:\n",
    "        processor_args = {\n",
    "            \"annotators\": \"tokenize,ssplit,pos,parse\",\n",
    "            \"tokenize.options\":\n",
    "                \"splitHyphenated=false,normalizedParentheses=false,normalizeOtherBrackets=false\",\n",
    "            \"tokenize.whitespace\": True,\n",
    "            \"ssplit.isOneSentence\": False,\n",
    "            \"outputFormat\": \"json\"\n",
    "        }\n",
    "    elif topology_builder == DependencyBasedGraphConstruction:\n",
    "        processor_args = {\n",
    "            \"annotators\": \"ssplit,tokenize,depparse\",\n",
    "            \"tokenize.options\":\n",
    "                \"splitHyphenated=false,normalizedParentheses=false,normalizeOtherBrackets=false\",\n",
    "            \"tokenize.whitespace\": True,\n",
    "            \"ssplit.isOneSentence\": True,\n",
    "            \"outputFormat\": \"json\"\n",
    "        }\n",
    "    else:\n",
    "            raise RuntimeError(\"Unknown topology_builder\")\n",
    "        \n",
    "    return processor_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07835d60-8263-4b55-9b92-a7e9cd5797fb",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "For now, to get the model to predict a sentence, a StanfordCoreNLP instance should be running at the same time as this function. If one is not already running, follow the same steps as in \"Configure and Run the Model\".\n",
    "\n",
    "Once StanfordCoreNLP is set up, a graph is constructed from the sentence using the topology builder defined in the config file (the same one used by the model). Then, the graph is fed into the model's `forward()` method (note that calling the model is the same as calling `model.forward()`). The prediction (output tensor) is put in the range of 0 to 1, and the `.item()` method is called on the prediction to return it as a standard Python number rather than a single-element list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd6ed6-84d3-4d75-9c23-648fbdde84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, config, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # Set up StanfordCoreNLP args\n",
    "    processor=stanfordcorenlp.StanfordCoreNLP(\"http://localhost\", port=9000, timeout=50000)\n",
    "    topology_builder = get_topology_builder(config)\n",
    "    processor_args = get_processor_args(topology_builder)\n",
    "    \n",
    "    # Build a graph of the sentence\n",
    "    graph = topology_builder.topology(sentence, processor, processor_args, merge_strategy=\"tailhead\", edge_strategy=None).to(config[\"device\"])\n",
    "    \n",
    "    # Add a \"token_id\" feature to each node to map the token to its representation in the model's vocab\n",
    "    tokens = [graph.node_attributes[node][\"token\"] for node in range(graph.get_node_num())]\n",
    "    id_list = [model.vocab.in_word_vocab.getIndex(tokens[i]) for i in range(graph.get_node_num())]\n",
    "    graph.node_features[\"token_id\"] = torch.tensor(id_list, dtype=torch.long).to(config[\"device\"])\n",
    "\n",
    "    # Convert the sentence's graph to a batch, since the model's forward function expects a batch of graphs\n",
    "    batch = to_batch([graph])\n",
    "    \n",
    "    # Pass the sentence through the model, put the values in the range [0, 1], and get rid of the batch dimension\n",
    "    pred = torch.sigmoid(model(batch, require_loss=False)).squeeze(0)\n",
    "    \n",
    "    # return the tensor without device or grad_fn info [neg pos]\n",
    "    return pred.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34934e6b-0532-44a6-835d-62e5f1ddfb52",
   "metadata": {},
   "source": [
    "Try the model on simple positive and negative sentences. Here, the meaning and sentiment are immediately clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b0d71-fee0-4e5a-9f25-a9fb2dd7da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(model, config, \"This movie is great\"))\n",
    "print(predict_sentiment(model, config, \"This movie is terrible\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97917278-870a-497b-b49e-098ac1af599c",
   "metadata": {},
   "source": [
    "Now, test the model on a variety of more complicated sentences. The meaning of these sentences depends on modifiers of words, so the model will have to recognize, for example, that \"not\" modifies \"terrible\", negating the negative meaning of the phrase \"not terrible\". The model still performs quite well in these situations. In fact, this model has superior performance over the convolutional model shown [here](https://github.com/bentrevett/pytorch-sentiment-analysis) and comparable performance to the transformer model in the same repo. Note that the transformer model in the linked repo achieves about a 10% higher validation/test accuracy than this model. Integrating transformers into this model is a potential way forward to achieve even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c49b23-b3d8-43df-8063-9834bb2347c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(model, config, \"This movie is not terrible, it's great\"))\n",
    "print(predict_sentiment(model, config, \"This movie is not great, it's terrible\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48734f1a-4911-4906-90c4-5c167d7df855",
   "metadata": {},
   "source": [
    "In these sentences, the meaning and sentiment of the sentences comes completely from some words modifying others. It is clear that \"far from terrible\" means something like \"quite good\" and is a positive phrase. However, the model seems to struggle in these scenarios, since there are no words in the sentence that directly reflect its sentiment. Rather, the words that carry strong sentiment are negated, but not directly. It could be difficult to tell that \"far from\" is rougly the same as \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f0320e-1ae2-4f74-a4fe-bdfee6e7bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(model, config, \"This movie is far from terrible\"))\n",
    "print(predict_sentiment(model, config, \"This movie is far from great\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e86f2b-037d-4fe4-a6e7-ac2078d59ac4",
   "metadata": {},
   "source": [
    "The sentences below get their sentiment from directly negating strong words. This still seems to confuse the model, as it still views \"not terrible\" as extremely likely to be negative and \"not great\" as extremely likely to be positive. To be fair, \"not great\" is not necessarily equivalent to \"terrible\". In fact, it could still be a positive phrase, as in \"The movie was decent, but it was not great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f641ef2-032e-424e-904c-e116cedb031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_sentiment(model, config, \"This movie is not terrible\"))\n",
    "print(predict_sentiment(model, config, \"This movie is not great\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2dcca-47f9-4a82-9b36-133e9b47ba41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
